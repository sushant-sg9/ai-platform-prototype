import { NextRequest, NextResponse } from 'next/server';

export interface ChatRequest {
  message: string;
  model: string;
  temperature?: number;
  maxTokens?: number;
}

export async function POST(request: NextRequest) {
  try {
    const { message, model, temperature = 0.7, maxTokens = 1000 } = await request.json() as ChatRequest;

    // Validate input
    if (!message || !model) {
      return NextResponse.json(
        { success: false, error: 'Message and model are required' },
        { status: 400 }
      );
    }

    // Create a ReadableStream for streaming response
    const stream = new ReadableStream({
      async start(controller) {
        try {
          // Simulate AI response streaming
          // In a real implementation, you'd call your AI provider's streaming API
          const simulatedResponse = await simulateStreamingResponse(message, model, controller);
          
          // Signal completion
          controller.enqueue(`data: {"type": "done", "content": ""}\n\n`);
          controller.close();
        } catch (error) {
          controller.enqueue(`data: {"type": "error", "content": "Failed to generate response"}\n\n`);
          controller.close();
        }
      },
    });

    return new Response(stream, {
      headers: {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
        'Access-Control-Allow-Origin': '*',
        'Access-Control-Allow-Methods': 'GET, POST, PUT, DELETE, OPTIONS',
        'Access-Control-Allow-Headers': 'Content-Type, Authorization',
      },
    });
  } catch (error) {
    console.error('Streaming error:', error);
    return NextResponse.json(
      { success: false, error: 'Internal server error' },
      { status: 500 }
    );
  }
}

async function simulateStreamingResponse(
  message: string, 
  model: string, 
  controller: ReadableStreamDefaultController
) {
  // Simulate different response styles based on model
  const responses = {
    'gpt-4': `Based on your query "${message}", I'll provide a comprehensive analysis. This response demonstrates how streaming works in real-time, showing each word as it's generated by the AI model. The streaming functionality creates a more engaging user experience by displaying content progressively rather than waiting for the complete response.`,
    'claude-3-sonnet': `I understand you're asking about "${message}". Let me walk you through this step by step. Streaming responses make AI interactions feel more natural and responsive, similar to how humans speak in conversation. Each token appears as soon as it's generated, creating an engaging dialogue experience.`,
    'gemini-pro': `Regarding "${message}" - here's my response with streaming enabled. This implementation uses Server-Sent Events to deliver content in real-time chunks. Users can see the AI "thinking" and formulating responses, which improves the perceived responsiveness of the application.`
  };

  const responseText = responses[model as keyof typeof responses] || 
    `Thank you for your message: "${message}". This is a streaming response that demonstrates real-time text generation. Each word appears as it's processed, creating a dynamic and interactive experience for users.`;

  const words = responseText.split(' ');
  
  for (let i = 0; i < words.length; i++) {
    const word = words[i];
    const isLast = i === words.length - 1;
    
    // Send each word with a small delay to simulate real streaming
    controller.enqueue(`data: {"type": "chunk", "content": "${word}${isLast ? '' : ' '}"}\n\n`);
    
    // Simulate processing time
    await new Promise(resolve => setTimeout(resolve, 50 + Math.random() * 100));
  }
}

export async function OPTIONS() {
  return new Response(null, {
    status: 200,
    headers: {
      'Access-Control-Allow-Origin': '*',
      'Access-Control-Allow-Methods': 'GET, POST, PUT, DELETE, OPTIONS',
      'Access-Control-Allow-Headers': 'Content-Type, Authorization',
    },
  });
}
